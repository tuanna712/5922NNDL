{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Tutorial 15: Acceleration and Optimization in Deep Learning\n",
        "\n",
        "```\n",
        "Course: CSCI 5922 Spring 2025, University of Colorado Boulder\n",
        "TA: Everley Tseng\n",
        "Email: Yu-Yun.Tseng@colorado.edu\n",
        "* AI assistant is used in making this tutorial\n",
        "```"
      ],
      "metadata": {
        "id": "TlonWLoy2QZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Sections:\n",
        "- Automatic Mixed Precision\n",
        "- Operator Fusion and Dynamic Quantization\n",
        "- Auto Fusion with `torch.compile()`\n",
        "- vLLM\n",
        "\n",
        "Objectives:\n",
        "- Learn how to implement deep learning optimization"
      ],
      "metadata": {
        "id": "-ysJIfGI2sNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Mixed Precision"
      ],
      "metadata": {
        "id": "M5tLnMXS2tG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use PyTorch's automatic mixed precision (AMP) to train models using lower precision. Specifically, PyTorch supports FP16 and FP8 for training, reducing memory usage and speeding up training. We will use a simple architecture for demonstration.\n",
        "\n",
        "1. Use `torch.amp.autocast` to enable mixed precision during the forward pass.\n",
        "2. Use `torch.amp.GradScaler` to scale the gradients during the backward pass.\n",
        "\n",
        "Note: This method applies on both CPU and GPU. On NVIDIA GPUs, AMP is widely used as it optimizes the computation better. For the following cell, we use a GPU runtime to demonstrate how to code AMP. You may switch the device to `cpu`.\n"
      ],
      "metadata": {
        "id": "sCCFKZD10IPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfnahW760Gt_",
        "outputId": "4b9d0e6c-7158-48e2-920c-682b3d35646f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "# Simple neural network for illustration\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = SimpleModel().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Scaler for mixed precision\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "# Training loop with mixed precision\n",
        "def train(model, data_loader):\n",
        "    model.train()\n",
        "    for data, target in data_loader:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Automatic Mixed Precision (AMP) Context Manager\n",
        "        with autocast('cuda'):\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Create a custom Dataset class to include both data and target\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data_size, input_dim, num_classes):\n",
        "        self.data = torch.randn(data_size, input_dim)  # Random input data\n",
        "        self.target = torch.randint(0, num_classes, (data_size,))  # Random target labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.target[idx]  # Return both data and target\n",
        "\n",
        "# Create a dataset and data loader\n",
        "dataset = SimpleDataset(data_size=100, input_dim=784, num_classes=10)\n",
        "train_loader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "# Run training loop\n",
        "train(model, train_loader)\n",
        "print('Finished training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operator Fusion And Dynamic Quantization"
      ],
      "metadata": {
        "id": "QJDDuj2w2rXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we explore the concepts of operator fusion and dynamic quantization to improve the efficiency of deep learning models. Operator fusion is a technique where multiple operations, such as convolution and activation (ReLU), are combined into a single operation to reduce computation and speed up inference.\n",
        "\n",
        "In this experiment, we apply operator fusion to a convolutional neural network (CNN) with three convolutional blocks, each consisting of a convolution layer followed by a ReLU activation. After fusing these operations, we apply dynamic quantization, which reduces the precision of the model weights from 32-bit floating point to 8-bit integers. This helps to reduce memory usage and increase inference speed, which is particularly important for deploying models on devices with limited resources.\n",
        "\n",
        "We will be using the `torch.quantization` package to perform dynamic quantization and the module fusion.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1xTtHteLDjiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Quantization applied via `quantize_dynamic()` works better on the CPU. Therefore, please switch to CPU for this section to see the effects."
      ],
      "metadata": {
        "id": "S752q6h13Mni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.quantization import quantize_dynamic, fuse_modules\n",
        "\n",
        "# Define a simple CNN model with 3 convolutional blocks\n",
        "class SimpleConvModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleConvModel().cpu()  # Move to CPU for quantization\n",
        "\n",
        "# Set the model to evaluation mode before quantization\n",
        "model.eval()\n",
        "\n",
        "# Fuse Conv2d + ReLU layers for each convolutional block\n",
        "model_fused = fuse_modules(model, [\n",
        "    ['conv1', 'relu1'],  # Fuse conv1 and relu1\n",
        "    ['conv2', 'relu2'],  # Fuse conv2 and relu2\n",
        "    ['conv3', 'relu3'],  # Fuse conv3 and relu3\n",
        "])\n",
        "\n",
        "# Apply 8-bit dynamic quantization to the model (only for layers that support it)\n",
        "quantized_model = quantize_dynamic(model_fused, {torch.nn.Conv2d}, dtype=torch.qint8)"
      ],
      "metadata": {
        "id": "d7PM2jRRFXW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With both the original model and the quantized model, let's run an inference to compare the latency."
      ],
      "metadata": {
        "id": "qguNhC1hHQNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Create a random input tensor (3x224x224 image)\n",
        "input_data = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Ensure both models are in evaluation mode\n",
        "model.eval()\n",
        "quantized_model.eval()\n",
        "\n",
        "# Test latency of the original model\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    original_output = model(input_data)  # Forward pass\n",
        "original_latency = time.time() - start_time\n",
        "\n",
        "# Test latency of the quantized model\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    quantized_output = quantized_model(input_data)  # Forward pass\n",
        "quantized_latency = time.time() - start_time\n",
        "\n",
        "# Print the latency results\n",
        "print(f\"Original model latency: {original_latency:.6f} seconds\")\n",
        "print(f\"Quantized model latency: {quantized_latency:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFlvDONoGAUB",
        "outputId": "ce945c30-2cc5-4e86-cb98-0a9ff4c4d684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model latency: 0.919242 seconds\n",
            "Quantized model latency: 0.698787 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto-Fusion with `torch.compile()`\n",
        "\n",
        "Instead of fusing modules manually using `fuse_modules`, you can instead use `torch.compile()`, which is a high-level API that automatically applies fusion and other optimizations to the model during execution. It looks at the computational graph and decides which operations can be fused or optimized based on the model's structure and the hardware you're running it on.\n",
        "\n",
        "In this section, we will test the GPT-like Architecture using this approach to demonstrate running a GPT model on a **CPU**."
      ],
      "metadata": {
        "id": "BeC-VuUl5omx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blocks in the GPT-like model contains a CausalSelfAttention layer and a MLP.\n",
        "\n",
        "FlashAttention is used in the CausalSelfAttention layer to speed up the attention computation. FlashAttention replaces the traditional attention mechanism and uses a highly optimized CUDA kernel.\n",
        "\n",
        "Code reference: https://github.com/karpathy/build-nanogpt/\n"
      ],
      "metadata": {
        "id": "pkjDhx1RKQmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch import fx\n",
        "from torch.nn import functional as F\n",
        "import torch._dynamo\n",
        "\n",
        "# CausalSelfAttention with FlashAttention\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        q, k, v = [x.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) for x in [q, k, v]]\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "# MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "# GPT Block\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "# GPT Model\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
        "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            'ln_f': nn.LayerNorm(config.n_embd),\n",
        "        })\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "# Example GPTConfig\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "# Initialize the model\n",
        "config = GPTConfig()\n",
        "model = GPT(config).cpu()\n",
        "\n",
        "# Apply torch.compile() for operator fusion\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "# FlashAttention integration - optimizing attention computation with FlashAttention\n",
        "# This is done in the CausalSelfAttention module where `F.scaled_dot_product_attention` is used.\n",
        "\n",
        "# Example of running a forward pass with torch.compile() and FlashAttention\n",
        "dummy_input = torch.randint(0, config.vocab_size, (1, config.block_size))\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    output = compiled_model(dummy_input)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Inference time with operator fusion and FlashAttention: {end_time - start_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwIuhUaf27Nl",
        "outputId": "4fd68698-96f9-4818-f8e5-4e96105618d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time with operator fusion and FlashAttention: 61.389983 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM"
      ],
      "metadata": {
        "id": "KFsmjTD5e5vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vLLM is an optimized library for faster inference of large language models. It supports distributed execution, optimized computation, and improved memory usage.\n",
        "- [Github](https://github.com/vllm-project/vllm)\n",
        "- [Documentation](https://docs.vllm.ai/en/latest/)\n",
        "\n",
        "To install `vllm`, run the command:\n",
        "```\n",
        "!pip install vllm\n",
        "```\n",
        "To install the package, make sure that your environment fits the [requirements](https://docs.vllm.ai/en/latest/getting_started/installation.html) and that the libraries are installed with correct [versions](https://github.com/vllm-project/vllm/tree/main/requirements). **The default environment on Colab might not fit the requirements**. Completing all required installations, including gcc/g++ and torch, might be time-consuming or exceed the storage or memory limit if using a default free runtime. We are unable to demo in this section, but we encourage you to try the installation on higher-level runtime or other platforms."
      ],
      "metadata": {
        "id": "7DtflQvie7dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vLLM supports generative and pooling models across various tasks. See this [page](https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models) for supported models. Sample code is in the cell below."
      ],
      "metadata": {
        "id": "aQIIpcg4fI-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import vllm\n",
        "\n",
        "# # Load model with vLLM (assuming you have a transformer model)\n",
        "# model = vllm.LLM(\"gpt2\")\n",
        "\n",
        "# # Run inference\n",
        "# input_text = \"Once upon a time, in a land far away,\"\n",
        "# output = model.generate(input_text)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "rm8h6uVkIsV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review"
      ],
      "metadata": {
        "id": "-_cuLpa930tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we implemented acceleration methods using PyTorch. As experienced in many tutorials and lab assignments, computing resources can be the limiting factor for deep learning projects. With the skills we learned in this tutorial, we are able to better utilize available CPU/GPU resources."
      ],
      "metadata": {
        "id": "irw502E631yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any questions and discussions regarding this tutorial, attend [TA office hours](https://docs.google.com/spreadsheets/d/1fzfTJpEF7RaUYRA_NGa3DkiazdQXVj7QNBbp6DrEZ3I/edit?usp=sharing) or create a post on [Piazza](https://piazza.com/colorado/spring2025/csci5922/home) :) See you in the next tutorial!\n",
        "\n",
        "\\- Everley"
      ],
      "metadata": {
        "id": "fra8Rz-C3zIB"
      }
    }
  ]
}